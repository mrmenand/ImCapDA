### ImCapDA: Fine-Tuning CLIP with Image Captions for Unsupervised Domain Adaptation

This repository provides a PyTorch implementation of **ImCapDA**, a method for fine-tuning the CLIP model using image captions for unsupervised domain adaptation. 

The dataset used in this work includes multiple domain-specific image-caption pairs located in the `data/` directory. 

This code is heavily inspired by and closely follows the implementation found in the [VLP-UDA repository](https://github.com/Wenlve-Zhou/VLP-UDA).

The complete code will be released in the near future.